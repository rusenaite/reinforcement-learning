# FunkcijÅ³ aproksimacija ir gilieji Q tinklai (DQN)

## ğŸ’¡ TeorinÄ— dalis

### FunkcijÅ³ aproksimacija

FunkcijÅ³ aproksimacija yra technika, naudojama stiprinamojo mokymosi (Reinforcement Learning, RL) kontekste, kai bÅ«senÅ³ erdvÄ— yra labai didelÄ— arba net begalinÄ—. Vietoje to, kad bÅ«tÅ³ kaupiamos visos bÅ«senÅ³-veiksmÅ³ poros Q reikÅ¡mÄ—s, naudojamas modelis (pvz., neuroninis tinklas), kuris apytiksliai apskaiÄiuoja Å¡ias vertes.

---

## ğŸ§  Deep Q-Networks (DQN)

### Kas yra DQN?

DQN â€“ tai gilusis Q-mokymosi metodas, kuris neuroniniu tinklu aproksimuojamos Q reikÅ¡mÄ—s, leidÅ¾ianÄios efektyviai veikti didelÄ—se bÅ«senÅ³ erdvÄ—se. Å is metodas iÅ¡populiarÄ—jo po 2015 m. pasirodÅ¾iusio darbo, kuriame DQN pasiekÄ— Å¾mogaus lygio rezultatÅ³ Å¾aidÅ¾iant Atari Å¾aidimus.

### Pagrindiniai komponentai:

- **Q tinklas:** Naudoja neuroninÄ¯ tinklÄ… Q(s, a) prognozei.
- **Patirties atkartojimas (Experience Replay):** Agentas saugo ankstesnius patyrimus ir mokosi iÅ¡ jÅ³ atsitiktine tvarka, kad sumaÅ¾intÅ³ sekos koreliacijÄ….
- **Tikslinis tinklas (Target Network):** Atskiras tinklas, kuris atnaujinamas retai, kad uÅ¾tikrintÅ³ stabilumÄ… mokymo metu.

---

## ğŸ” Double DQN

### Problema:
Klasikinis DQN pervertina Q reikÅ¡mes, nes tas pats tinklas naudojamas ir veiksmo parinkimui, ir Ä¯vertinimui.

### Sprendimas:
Double DQN naudoja du tinklus:
- Vienas parenka geriausiÄ… veiksmÄ… (`argmax`),
- Kitas Ä¯vertina to veiksmo reikÅ¡mÄ™ naudodamas atskirus svorius (`Î¸â»`).

### FormulÄ—:
```
Q(s, a) = r + Î³ * Q_target(s', argmax_a' Q(s', a'; Î¸), Î¸â»)
```

---

## ğŸ§¾ Dueling DQN

### Problema:
Kai kuriose bÅ«senose veiksmo pasirinkimas neturi didelÄ—s reikÅ¡mÄ—s, bet klasikinis DQN vis tiek modeliuoja visas Q reikÅ¡mes vienodai.

### Sprendimas:
Dueling DQN tinklo architektÅ«ra atskiria:
- **V(s):** bÅ«sena vertÄ—,
- **A(s, a):** veiksmo pranaÅ¡umas.

### FormulÄ—:
```
Q(s, a) = V(s) + A(s, a) - max_a' A(s, a')
```

---

## ğŸ“ Apibendrinimas

- **DQN:** sprendÅ¾ia dideliÅ³ erdviÅ³ problemas naudodamas neuroninius tinklus.
- **Double DQN:** sumaÅ¾ina Q reikÅ¡miÅ³ pervertinimÄ….
- **Dueling DQN:** leidÅ¾ia tinklui geriau Ä¯vertinti bÅ«senas net kai veiksmo pasirinkimas nÄ—ra svarbus.

---

## ğŸ§ª Praktiniai DQN panaudojimo atvejai

### 1. Å½aidimai (pvz., Atari)
- **Breakout, Space Invaders, Pong, Ms. Pac-Man**
- Agentas mokosi Å¾aisti siekdamas maksimizuoti rezultatÄ….

### 2. AutonominÄ—s transporto priemonÄ—s
- **Veiksmai:** vairavimas, stabdymas, sukimas
- **Atlygis:** pasiektas tikslas, iÅ¡vengtos kliÅ«tys

### 3. RobotÅ³ valdymas
- **UÅ¾duotys:** objektÅ³ paÄ—mimas, judÄ—jimas
- **Atlygis:** sÄ—kmingas uÅ¾duoties atlikimas

### 4. Finansai
- **Veiksmai:** pirkti, parduoti, laikyti
- **Atlygis:** pelnas ar nuostolis

### 5. SistemÅ³ ir tinklÅ³ optimizavimas
- **BÅ«sena:** tinklo srautas, serverio apkrova
- **Veiksmai:** resursÅ³ paskirstymas
- **Atlygis:** optimizuota veikla

---

## ğŸ“š Naudingi Å¡altiniai

- [IEEE publikacija](https://ieeexplore.ieee.org/abstract/document/10854435)
- Mnih et al., 2015 â€“ "Human-level control through deep reinforcement learning"
- Wang et al., 2016 â€“ "Dueling Network Architectures for Deep Reinforcement Learning"
- [Kaggle notebook: Dueling Double DQN](https://www.kaggle.com/code/masurte/dueling-double-dqn)
