{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIUL0YInLWOC",
        "outputId": "c9e80040-df23-43f8-f9f4-a128386e25cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vertƒós funkcija:\n",
            "[[0.95099005 0.96059601 0.970299   0.96059601]\n",
            " [0.96059601 0.         0.9801     0.        ]\n",
            " [0.970299   0.9801     0.99       0.        ]\n",
            " [0.         0.99       1.         0.        ]]\n",
            "\n",
            "Optimali politika (veiksm≈≥ indeksai):\n",
            "[[1. 2. 1. 0.]\n",
            " [1. 0. 1. 0.]\n",
            " [2. 1. 1. 0.]\n",
            " [0. 2. 2. 0.]]\n",
            "\n",
            "Politika simboliais:\n",
            "‚Üì ‚Üí ‚Üì ‚Üê\n",
            "‚Üì ‚Üê ‚Üì ‚Üê\n",
            "‚Üí ‚Üì ‚Üì ‚Üê\n",
            "‚Üê ‚Üí ‚Üí ‚Üê\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# Dinaminio programavimo taikymas: Vertƒós iteracija FrozenLake aplinkoje\n",
        "\n",
        "# Reikalingos bibliotekos\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Aplinkos paruo≈°imas\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)  # Naudojame deterministinƒô versijƒÖ, STABILIAM ELGESIUI\n",
        "env.reset()\n",
        "\n",
        "# Vertƒós iteracijos algoritmas\n",
        "def value_iteration(env, gamma=0.99, theta=1e-8):\n",
        "    value_table = np.zeros(env.observation_space.n)  # Pradinƒós vertƒós\n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in range(env.observation_space.n):\n",
        "            action_values = []\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * value_table[next_state])\n",
        "                action_values.append(q_value)\n",
        "\n",
        "            max_value = max(action_values)\n",
        "            delta = max(delta, abs(value_table[state] - max_value))\n",
        "            value_table[state] = max_value\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    # Optimalios politikos sudarymas\n",
        "    for state in range(env.observation_space.n):\n",
        "        action_values = []\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * value_table[next_state])\n",
        "            action_values.append(q_value)\n",
        "        policy[state] = np.argmax(action_values)\n",
        "\n",
        "    return value_table, policy\n",
        "\n",
        "# Vertƒós iteracijos vykdymas\n",
        "value_table, optimal_policy = value_iteration(env)\n",
        "\n",
        "print(\"Vertƒós funkcija:\")\n",
        "print(value_table.reshape(4, 4))  # Atvaizduojame 4x4 tinklelyje\n",
        "\n",
        "print(\"\\nOptimali politika (veiksm≈≥ indeksai):\")\n",
        "print(optimal_policy.reshape(4, 4))\n",
        "\n",
        "# Politikos simbolinƒó vizualizacija\n",
        "action_symbols = ['‚Üê', '‚Üì', '‚Üí', '‚Üë']\n",
        "policy_symbols = [action_symbols[int(a)] for a in optimal_policy]\n",
        "policy_grid = np.array(policy_symbols).reshape(4, 4)\n",
        "print(\"\\nPolitika simboliais:\")\n",
        "for row in policy_grid:\n",
        "    print(' '.join(row))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zngMAWF1UBcR"
      },
      "source": [
        "B≈´senos su auk≈°tesnƒómis vertƒómis rodo optimalius veiksmus, kad pasiektume tikslƒÖ, ir tokiose vietose atlikti optimal≈´s veiksmai! (I≈°eiti i≈° labirinto, pasiekti pabaigƒÖ ir t.t.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THMUX6mcUjca"
      },
      "source": [
        "Kuo arƒçiau b≈´senos su tikslo vieta, tuo didesnƒós vertƒós jos turi, nes tikslas yra pasiekiamas i≈° j≈≥ su didesne tikimybe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3uqMDcpLkvs"
      },
      "source": [
        "SKIRTINGOS GAMMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKZlF1dCWabQ"
      },
      "source": [
        "Gamma (Œ≥) yra nuolaidos (angl. discount factor) parametras, kuris yra svarbus daugelyje reinforcement learning (RL) algoritm≈≥, ƒØskaitant vertƒós iteracijƒÖ ir politikos iteracijƒÖ.\n",
        "\n",
        "Jis nusako, kaip agentas vertina ateities atlygius, palyginti su dabartiniais atlygiais.\n",
        "\n",
        "KƒÖ rei≈°kia gamma?\n",
        "ùõæ\n",
        "Œ≥ reik≈°mƒó yra intervale nuo 0 iki 1.\n",
        "\n",
        "ùõæ\n",
        "=\n",
        "0\n",
        "Œ≥=0 rei≈°kia, kad agentas visi≈°kai ignoruoja ateities atlygius ir labiausiai vertina tik dabartinius atlygius.\n",
        "\n",
        "ùõæ\n",
        "=\n",
        "1\n",
        "Œ≥=1 rei≈°kia, kad agentas visi≈°kai vertina tiek dabartinius, tiek ateities atlygius. Kiekvienas b≈´simos naudos gabalas yra vertinamas taip pat, kaip ir dabartinƒó nauda.\n",
        "\n",
        "0 <\n",
        "ùõæ\n",
        "Œ≥ < 1 rei≈°kia, kad agentas vertina tiek dabartinius, tiek ateities atlygius, taƒçiau ateities atlygiai yra ≈°iek tiek ma≈æiau vertinami nei dabartiniai."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA5YhBjhU_Ex",
        "outputId": "ef9724a8-f80a-48f7-ba72-00f0760f9956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gamma = 0.9\n",
            "Vertƒós funkcija:\n",
            "[[0.59049 0.6561  0.729   0.6561 ]\n",
            " [0.6561  0.      0.81    0.     ]\n",
            " [0.729   0.81    0.9     0.     ]\n",
            " [0.      0.9     1.      0.     ]]\n",
            "\n",
            "Optimali politika (veiksm≈≥ indeksai):\n",
            "[[1. 2. 1. 0.]\n",
            " [1. 0. 1. 0.]\n",
            " [2. 1. 1. 0.]\n",
            " [0. 2. 2. 0.]]\n",
            "Gamma = 0.95\n",
            "Vertƒós funkcija:\n",
            "[[0.77378094 0.81450625 0.857375   0.81450625]\n",
            " [0.81450625 0.         0.9025     0.        ]\n",
            " [0.857375   0.9025     0.95       0.        ]\n",
            " [0.         0.95       1.         0.        ]]\n",
            "\n",
            "Optimali politika (veiksm≈≥ indeksai):\n",
            "[[1. 2. 1. 0.]\n",
            " [1. 0. 1. 0.]\n",
            " [2. 1. 1. 0.]\n",
            " [0. 2. 2. 0.]]\n",
            "Gamma = 0.99\n",
            "Vertƒós funkcija:\n",
            "[[0.95099005 0.96059601 0.970299   0.96059601]\n",
            " [0.96059601 0.         0.9801     0.        ]\n",
            " [0.970299   0.9801     0.99       0.        ]\n",
            " [0.         0.99       1.         0.        ]]\n",
            "\n",
            "Optimali politika (veiksm≈≥ indeksai):\n",
            "[[1. 2. 1. 0.]\n",
            " [1. 0. 1. 0.]\n",
            " [2. 1. 1. 0.]\n",
            " [0. 2. 2. 0.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Aplinkos paruo≈°imas\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "env.reset()\n",
        "\n",
        "# Vertƒós iteracijos algoritmas\n",
        "def value_iteration(env, gamma=0.99, theta=1e-8):\n",
        "    value_table = np.zeros(env.observation_space.n)\n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in range(env.observation_space.n):\n",
        "            action_values = []\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * value_table[next_state])\n",
        "                action_values.append(q_value)\n",
        "\n",
        "            max_value = max(action_values)\n",
        "            delta = max(delta, abs(value_table[state] - max_value))\n",
        "            value_table[state] = max_value\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    # Optimalios politikos sudarymas\n",
        "    for state in range(env.observation_space.n):\n",
        "        action_values = []\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * value_table[next_state])\n",
        "            action_values.append(q_value)\n",
        "        policy[state] = np.argmax(action_values)\n",
        "\n",
        "    return value_table, policy\n",
        "\n",
        "# Eksperimentavimas su skirtingais `gamma` reik≈°mƒómis\n",
        "gamma_values = [0.9, 0.95, 0.99]\n",
        "for gamma in gamma_values:\n",
        "    print(f\"Gamma = {gamma}\")\n",
        "    value_table, optimal_policy = value_iteration(env, gamma=gamma)\n",
        "    print(\"Vertƒós funkcija:\")\n",
        "    print(value_table.reshape(4, 4))  # Atvaizduojame 4x4 tinklelyje\n",
        "    print(\"\\nOptimali politika (veiksm≈≥ indeksai):\")\n",
        "    print(optimal_policy.reshape(4, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGbLg5lkWvuZ"
      },
      "source": [
        "Ma≈æas\n",
        "ùõæ\n",
        "Œ≥ gali priversti agentƒÖ priimti ‚Äûtrumpalaikius‚Äú sprendimus, da≈ænai pasitenkinant ma≈æais, taƒçiau greitais apdovanojimais.\n",
        "\n",
        "Didelis\n",
        "ùõæ\n",
        "Œ≥ skatina agentƒÖ priimti ‚Äûilgo laikotarpio‚Äú sprendimus, kurie da≈ænai reikalauja ‚Äûkantrybƒós‚Äú ir veda ƒØ didesnius apdovanojimus ateityje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQp6KFJrVTt4",
        "outputId": "f96ed1ff-2614-4384-9992-02cc4228b1bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pradinis ta≈°kas: 7\n",
            "Vertƒós funkcija:\n",
            "[[0.95099005 0.96059601 0.970299   0.96059601]\n",
            " [0.96059601 0.         0.9801     0.        ]\n",
            " [0.970299   0.9801     0.99       0.        ]\n",
            " [0.         0.99       1.         0.        ]]\n",
            "\n",
            "Optimali politika (veiksm≈≥ indeksai):\n",
            "[[1. 2. 1. 0.]\n",
            " [1. 0. 1. 0.]\n",
            " [2. 1. 1. 0.]\n",
            " [0. 2. 2. 0.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "\n",
        "# Aplinkos paruo≈°imas\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "# Atsitiktinis pradinis ta≈°kas\n",
        "def random_start_state():\n",
        "    return random.randint(0, env.observation_space.n - 1)\n",
        "\n",
        "# Vertƒós iteracijos algoritmas su atsitiktiniu prad≈æios ta≈°ku\n",
        "def value_iteration_random_start(env, gamma=0.99, theta=1e-8):\n",
        "    value_table = np.zeros(env.observation_space.n)\n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    # Nustatome atsitiktinƒØ prad≈æios ta≈°kƒÖ\n",
        "    start_state = random_start_state()\n",
        "    print(f\"Pradinis ta≈°kas: {start_state}\")\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in range(env.observation_space.n):\n",
        "            action_values = []\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * value_table[next_state])\n",
        "                action_values.append(q_value)\n",
        "\n",
        "            max_value = max(action_values)\n",
        "            delta = max(delta, abs(value_table[state] - max_value))\n",
        "            value_table[state] = max_value\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    # Optimalios politikos sudarymas\n",
        "    for state in range(env.observation_space.n):\n",
        "        action_values = []\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * value_table[next_state])\n",
        "            action_values.append(q_value)\n",
        "        policy[state] = np.argmax(action_values)\n",
        "\n",
        "    return value_table, policy\n",
        "\n",
        "# Vertƒós iteracijos vykdymas su atsitiktiniu prad≈æios ta≈°ku\n",
        "value_table, optimal_policy = value_iteration_random_start(env)\n",
        "\n",
        "print(\"Vertƒós funkcija:\")\n",
        "print(value_table.reshape(4, 4))\n",
        "print(\"\\nOptimali politika (veiksm≈≥ indeksai):\")\n",
        "print(optimal_policy.reshape(4, 4))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
